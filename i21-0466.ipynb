{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21i-0466 Shahmeer Ali Akhtar\n",
    "### LOADING LIBRARIES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BytePairEncoding Class\n",
    "\n",
    "### Constructor\n",
    "- `__init__(self, vocab_size=1000)`: Initializes the BPE encoder with a specified vocabulary size. The default size is set to 1000.\n",
    "\n",
    "### Attributes\n",
    "- `vocab_size`: The maximum size of the vocabulary for BPE.\n",
    "- `bpe_vocab`: Dictionary to store the BPE vocabulary after training.\n",
    "- `normalization_dict`: Dictionary for normalizing specific words during preprocessing.\n",
    "\n",
    "### Methods\n",
    "\n",
    "#### `preprocess_text(self, text)`\n",
    "- Preprocesses the input text by removing numbers, converting to lowercase, removing non-alphabetic characters, and normalizing text based on the `normalization_dict`.\n",
    "\n",
    "#### `tokenize(self, text)`\n",
    "- Splits the preprocessed text into tokens based on whitespace.\n",
    "\n",
    "#### `get_vocab(self, texts)`\n",
    "- Generates a frequency dictionary of tokens from the list of input texts.\n",
    "\n",
    "#### `get_stats(self, vocab)`\n",
    "- Calculates the frequency of adjacent symbol pairs in the vocabulary.\n",
    "\n",
    "#### `merge_vocab(self, pair, vocab)`\n",
    "- Merges the most frequent pair of symbols in the vocabulary to create a new vocabulary with the merged symbol.\n",
    "\n",
    "#### `train(self, texts)`\n",
    "- Trains the BPE model using the provided texts by iteratively merging the most frequent pairs until the vocabulary size is reached or no more pairs can be merged.\n",
    "\n",
    "#### `print_vocab_size(self)`\n",
    "- Prints the total number of unique entries in the BPE vocabulary.\n",
    "\n",
    "#### `encode(self, text)`\n",
    "- Encodes a text into subwords based on the trained BPE vocabulary. Tokens not found in the vocabulary are marked as `<UNK>`.\n",
    "\n",
    "#### `decode(self, encoded_text)`\n",
    "- Decodes the list of encoded tokens back into a string.\n",
    "\n",
    "#### `evaluate_bpe(self, test_texts)`\n",
    "- Evaluates the BPE model on test texts to calculate the ratio of unknown tokens, the coverage ratio of known tokens, and the unique token coverage.\n",
    "\n",
    "#### `save_model(self, file_path)`\n",
    "- Saves the trained BPE vocabulary to a file using Python's `pickle` module.\n",
    "\n",
    "#### `load_model(self, file_path)`\n",
    "- Loads a BPE vocabulary from a file into the `bpe_vocab` attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BytePairEncoding:\n",
    "    def __init__(self, vocab_size=1000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.bpe_vocab = {}\n",
    "        self.normalization_dict = {\n",
    "            \"mujhe\": \"mujhay\",\n",
    "            \"kya\": \"kia\",\n",
    "            \"mei\":\"ma\",\n",
    "            \"main\":\"ma\",\n",
    "            \"mai\":\"ma\"\n",
    "        }\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        text = re.sub(r'^\\d+\\.', '', text)\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        for key, value in self.normalization_dict.items():\n",
    "            text = text.replace(key, value)\n",
    "        return text\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return text.split()\n",
    "\n",
    "    def get_vocab(self, texts):\n",
    "        vocab = Counter()\n",
    "        for text in texts:\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                vocab[token] += 1\n",
    "        return vocab\n",
    "\n",
    "    def get_stats(self, vocab):\n",
    "        pairs = Counter()\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pair = (symbols[i], symbols[i + 1])\n",
    "                pairs[pair] += freq\n",
    "        return pairs\n",
    "\n",
    "    def merge_vocab(self, pair, vocab):\n",
    "        replacement = ''.join(pair)\n",
    "        new_vocab = {}\n",
    "        bigram = ' '.join(pair)\n",
    "        for word in vocab:\n",
    "            new_word = re.sub(r'\\b' + re.escape(bigram) + r'\\b', replacement, word)\n",
    "            new_vocab[new_word] = vocab[word]\n",
    "        return new_vocab\n",
    "\n",
    "    def train(self, texts):\n",
    "        vocab = self.get_vocab(texts)\n",
    "        self.bpe_vocab = vocab.copy()\n",
    "        for i in range(self.vocab_size):\n",
    "            pairs = self.get_stats(self.bpe_vocab)\n",
    "            if not pairs:\n",
    "                break\n",
    "            most_frequent_pair = pairs.most_common(1)[0][0]\n",
    "            self.bpe_vocab = self.merge_vocab(most_frequent_pair, self.bpe_vocab)\n",
    "\n",
    "    def print_vocab_size(self):\n",
    "        print(\"Total unique vocabulary size:\", len(self.bpe_vocab))\n",
    "\n",
    "    def encode(self, text):\n",
    "        words = self.tokenize(text)\n",
    "        encoded_text = []\n",
    "        for word in words:\n",
    "            if word in self.bpe_vocab:\n",
    "                encoded_text.append(word)\n",
    "            else:\n",
    "                encoded_text.append('<UNK>')\n",
    "        return encoded_text\n",
    "\n",
    "    def decode(self, encoded_text):\n",
    "        return ' '.join(encoded_text)\n",
    "\n",
    "    def evaluate_bpe(self, test_texts):\n",
    "        total_tokens = 0\n",
    "        unk_tokens = 0\n",
    "        covered_tokens = 0\n",
    "        vocab_tokens = set(self.bpe_vocab.keys())\n",
    "\n",
    "        test_vocab = Counter()\n",
    "        for text in test_texts:\n",
    "            tokens = self.tokenize(text)\n",
    "            test_vocab.update(tokens)\n",
    "\n",
    "        for token, freq in test_vocab.items():\n",
    "            total_tokens += freq\n",
    "            if token in vocab_tokens:\n",
    "                covered_tokens += freq\n",
    "            else:\n",
    "                unk_tokens += freq\n",
    "\n",
    "        unk_ratio = unk_tokens / total_tokens if total_tokens > 0 else 0\n",
    "        coverage_ratio = covered_tokens / total_tokens if total_tokens > 0 else 0\n",
    "        unique_coverage = len(vocab_tokens.intersection(test_vocab.keys())) / len(test_vocab.keys())\n",
    "\n",
    "        print(f\"Total tokens: {total_tokens}\")\n",
    "        print(f\"Unknown token occurrences: {unk_tokens}\")\n",
    "        print(f\"Unknown token ratio: {unk_ratio:.4f}\")\n",
    "        print(f\"Coverage ratio: {coverage_ratio:.4f}\")\n",
    "        print(f\"Unique token coverage: {unique_coverage:.4f}\")\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        with open(file_path, 'wb') as file:\n",
    "            pickle.dump(self.bpe_vocab, file)\n",
    "\n",
    "    def load_model(self, file_path):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            self.bpe_vocab = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dairies from the folder\n",
    "And displaying the total vocabulary size that the model has learned \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique vocabulary size: 2426\n"
     ]
    }
   ],
   "source": [
    "def load_texts_from_folder(folder_path):\n",
    "    file_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "    texts = []\n",
    "    for path in file_paths:\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                processed_text = bpe_processor.preprocess_text(text)\n",
    "                texts.append(processed_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {path}: {e}\")\n",
    "    return texts\n",
    "folder_path = \"D:\\\\NLP\\\\i21-0466_A2\\\\corpus\" #D:\\NLP\\i21-0466_A2\\corpus\n",
    "bpe_processor = BytePairEncoding(vocab_size=1000)\n",
    "processed_texts = load_texts_from_folder(folder_path)\n",
    "bpe_processor.train(processed_texts)\n",
    "bpe_processor.print_vocab_size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model and then loading it for further use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"bpe_model.pkl\"\n",
    "bpe_processor.save_model(model_path)\n",
    "bpe_processor.load_model(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 1604\n",
      "Unknown token occurrences: 418\n",
      "Unknown token ratio: 0.2606\n",
      "Coverage ratio: 0.7394\n",
      "Unique token coverage: 0.5469\n"
     ]
    }
   ],
   "source": [
    "test_folder_path = 'D:\\\\NLP\\\\i21-0466_A2\\\\test'\n",
    "\n",
    "test_files = [f for f in os.listdir(test_folder_path) if f.endswith('.txt')]\n",
    "\n",
    "test_texts = []\n",
    "for file_name in test_files:\n",
    "    with open(os.path.join(test_folder_path, file_name), 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        test_texts.append(text)\n",
    "\n",
    "bpe_processor.evaluate_bpe(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Criteria\n",
    "\n",
    "The `evaluate_bpe` method assesses the performance of the BPE model using the following metrics:\n",
    "\n",
    "- **Total Tokens**: Counts all tokens in the test texts. This metric indicates the size of the data being evaluated.\n",
    "\n",
    "- **Unknown Token Occurrences**: Counts how many tokens from the test texts are not recognized by the model's vocabulary. A lower number suggests better model performance.\n",
    "\n",
    "- **Unknown Token Ratio**: Measures the proportion of tokens that are unrecognized by the model compared to the total tokens. A lower ratio indicates better coverage of the test data by the model.\n",
    "\n",
    "- **Coverage Ratio**: Indicates the percentage of total tokens that the model's vocabulary can recognize and represent. Higher coverage ratios reflect better model accuracy.\n",
    "\n",
    "- **Unique Token Coverage**: Evaluates how many unique tokens from the test texts are covered by the model's vocabulary. Higher values show a more diverse and effective vocabulary.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
